#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import DL_for_HTT.common.NN_settings as NN_default_settings

from optparse import OptionParser
usage = "usage: %prog [options] <input>"
parser = OptionParser(usage=usage)
parser.add_option("-o", "--output", dest = "output",
                  default = "")
parser.add_option("-m", "--minmass", dest = "min_mass",
                  default = NN_default_settings.min_mass)
parser.add_option("-M", "--maxmass", dest = "max_mass",
                  default = NN_default_settings.max_mass)
parser.add_option("-c", "--channels", dest = "channels",
                  default = NN_default_settings.channels)
parser.add_option("-d", "--max_depth", dest = "max_depth",
                  default = 5) # max depth for trees
parser.add_option("-e", "--eta", dest = "eta",
                  default = 0.1) # learning rate
parser.add_option("-n", "--n_estimators", dest = "n_estimators",
                  default = 1000) # max number of estimators
parser.add_option("-s", "--early_stopping_rounds", dest = "early_stopping_rounds",
                  default = 5) # early stopping for estimators
parser.add_option("-E", "--eval", dest = "eval",
                  default = 'rmse')
parser.add_option("-g", "--gamma", dest = "gamma",
                  default = 0)
parser.add_option("-w", "--min_child_weight", dest = "min_child_weight",
                  default = 1)
parser.add_option("-j", "--n_jobs", dest = "n_jobs",
                  default = 1)
parser.add_option("-O", "--objective", dest = "objective",
                  default = 'squarederror')
parser.add_option("-i", "--model_inputs", dest = "model_inputs",
                  default = 'with_METcov_j1j2jr_Npu')

(options,args) = parser.parse_args()

min_mass = float(options.min_mass)
max_mass = float(options.max_mass)
options.max_depth = int(options.max_depth)
options.eta = float(options.eta)
options.n_estimators = int(options.n_estimators)
if options.early_stopping_rounds not in [None, 'None']:
    options.early_stopping_rounds = int(options.early_stopping_rounds)
else:
    options.early_stopping_rounds = None
options.gamma = float(options.gamma)
options.n_jobs = int(options.n_jobs)
    
input_file = args[0]

print("Selected options are the following:")
for option in ["output", "min_mass", "max_mass", "channels", "max_depth", "eta", "n_estimators", "eval", "gamma", "early_stopping_rounds", "min_child_weight", "n_jobs", "objective"]:
    print("\t{}\t{}".format(option, getattr(options, option)))

print("Input file:")
print("\t{}".format(input_file))

import os
from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load data
df = pd.read_hdf(input_file)

# define target and input variables
target = NN_default_settings.target

print("Target: {}".format(target))

model_inputs_file = __import__('DL_for_HTT.common.model_inputs.{}'.format(options.model_inputs), fromlist=[''])
model_inputs = model_inputs_file.inputs

if "N_neutrinos_reco" in model_inputs:
    df["N_neutrinos_reco"] = 2*np.ones(len(df["channel_reco"]), dtype='int')
    df.loc[(df["channel_reco"] == "mt"), ["N_neutrinos_reco"]] = 3
    df.loc[(df["channel_reco"] == "et"), ["N_neutrinos_reco"]] = 3
    df.loc[(df["channel_reco"] == "mm"), ["N_neutrinos_reco"]] = 4
    df.loc[(df["channel_reco"] == "em"), ["N_neutrinos_reco"]] = 4
    df.loc[(df["channel_reco"] == "ee"), ["N_neutrinos_reco"]] = 4

if "tau1_px_reco" in model_inputs:
    for ptc in ["tau1", "tau2", "jet1", "jet2", "remaining_jets", "MET", "PuppiMET"]:
        if "{}_eta_reco".format(ptc) in df.keys():
            df["{}_pz_reco".format(ptc)] = df["{}_pt_reco".format(ptc)] * np.sinh(df["{}_eta_reco".format(ptc)])
        df["{}_px_reco".format(ptc)] = df["{}_pt_reco".format(ptc)] * np.cos(df["{}_phi_reco".format(ptc)])
        df["{}_py_reco".format(ptc)] = df["{}_pt_reco".format(ptc)] * np.sin(df["{}_phi_reco".format(ptc)])

print("Model_Inputs:")
for i in model_inputs:
    print("\t{}".format(i))
    
# custom objective function
def low_mass(y_true, y_pred):

    grad = y_pred - y_true
    hess = np.ones(len(grad))

    low_mass_cut = 400
    factor = 2*(low_mass_cut/y_true)
    grad[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]
    hess[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]

    return grad, hess
    
def linear_low_mass(y_true, y_pred):

    grad = y_pred - y_true
    hess = np.ones(len(grad))

    max_factor = 50
    max_m = 200
    min_m = 50
    factor = (max_factor-1) * (max_m - y_true) / (max_m - min_m) + 1
    grad[y_true<max_m] *= factor[y_true<max_m]
    hess[y_true<max_m] *= factor[y_true<max_m]

    return grad, hess
    
def custom_low_mass(y_true, y_pred):

    grad = y_pred/y_true-1
    hess = 1./y_true

    for low_mass_cut in [100, 200, 300, 400]:
        factor = low_mass_cut/y_true
        grad[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]
        hess[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]

    return grad, hess

def get_custom(low_mass_cuts = [100, 200, 300, 300]):
    
    def custom(y_true, y_pred, low_mass_cuts = low_mass_cuts):

        grad = y_pred - y_true
        hess = np.ones(len(grad))

        for low_mass_cut in low_mass_cuts:
            factor = low_mass_cut/y_true
            grad[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]
            hess[y_true<low_mass_cut] *= factor[y_true<low_mass_cut]
            
        return grad, hess
    
    return custom
    
objectives = {
    'custom_low_mass' : custom_low_mass,
    'linear_low_mass' : linear_low_mass,
    'low_mass' : low_mass,
    'squarederror' : "reg:squarederror",
    'gamma' : "reg:gamma",
    'tweedie' : "reg:tweedie",
    }

low_mass_cuts_list = [
    [100],
    [100,200],
    [100,200,200],
    [100,200,300],
    [100,200,300,300],
    [100,200,300,400],
    [400],
    [400,400],
    [400,400,400],
    [400,400,400,400],
]
for low_mass_cuts in low_mass_cuts_list:
    objectives[''.join(['custom_low_mass_'] + [str(int(low_mass_cut/100)) for low_mass_cut in low_mass_cuts])] = get_custom(low_mass_cuts)

objectives['custom_low_mass_Higgs1'] = get_custom([125])
objectives['custom_low_mass_Higgs2'] = get_custom([125,125*2])
objectives['custom_low_mass_Higgs3'] = get_custom([125,125*2,125*3])
objectives['custom_low_mass_Higgs4'] = get_custom([125,125*2,125*3,125*4])
objectives['custom_low_mass_HiggsD3'] = get_custom([125*2,125*2,125*2])
objectives['custom_low_mass_HiggsD4'] = get_custom([125*2,125*2,125*2,125*2])

def XGB_make_train_predict(df, model_inputs, channel = "inclusive"):

    params = {
        'max_depth' : options.max_depth,
        'learning_rate' : options.eta,
        'eta' : options.eta, # duplicate for the name
        'n_estimators' : options.n_estimators,
        'gamma' : options.gamma,
        'min_child_weight' : options.min_child_weight,
        'n_jobs' : options.n_jobs,
        'objective' : objectives[options.objective],
        'eval_metric' : options.eval,
        'eval' : options.eval, # duplicate for the name
        'early_stopping_rounds' : options.early_stopping_rounds,
        'es' : options.early_stopping_rounds,
    }

    XGBname_items = []
    if options.output != "":
        XGBname_items.append(options.output)
    XGBname_items.append(channel)
    for param in ['max_depth', 'eta', 'n_estimators', 'es', 'gamma', 'min_child_weight', 'eval']:
        XGBname_items += [param, str(params[param])]
    XGBname = "-".join(XGBname_items)
    XGBname += "-objective-{}".format(options.objective)

    print(XGBname)

    df_select = df
    df_select = df_select.loc[(df_select[target] >= min_mass) & (df_select[target] <= max_mass)]

    if channel in set(df_select['channel_reco']):
        df_select = df_select.loc[(df_select['channel_reco'] == channel)]
    elif channel == "lt":
        df_select = df_select.loc[(df_select['channel_reco'] == "mt") | (df_select['channel_reco'] == "et")]
    elif channel == "ll":
        df_select = df_select.loc[(df_select['channel_reco'] == "mm") | (df_select['channel_reco'] == "em") | (df_select['channel_reco'] == "ee")]

    df_x_train = df_select.loc[(df_select['is_train'] == 1), model_inputs]
    df_y_train = df_select.loc[(df_select['is_train'] == 1), [target, "sample_weight"]]
    df_x_valid = df_select.loc[(df_select['is_valid'] == 1), model_inputs]
    df_y_valid = df_select.loc[(df_select['is_valid'] == 1), [target, "sample_weight"]]
    df_x_test = df_select.loc[(df_select['is_test'] == 1), model_inputs]
    df_y_test = df_select.loc[(df_select['is_test'] == 1), [target, "sample_weight"]]

    print('Size of training set: ', len(df_x_train))
    print('Size of valid set: ', len(df_x_valid))

    if len(df_x_train) == 0 or len(df_x_valid) == 0:
        print("Empty set, aborting...")
        return None, False

    # Create XGBRegressor
    bst = XGBRegressor(**params)
    
    # Create subsamples for evaluation
    eval_set = [(df_x_train, df_y_train[target]), (df_x_test, df_y_test[target]), (df_x_valid, df_y_valid[target])]
    eval_order = ['train', 'test', 'valid'] # to get back the order of eval_set later

    # Train XGBRegressor
    bst.fit(
        df_x_train, df_y_train[target],
        verbose=True,
        eval_set = eval_set,
        eval_metric = params['eval_metric'],
        early_stopping_rounds = params['early_stopping_rounds'],
        sample_weight = np.r_[df_y_train["sample_weight"]],
    )
    evals_result = bst.evals_result()
    evals_result['train'] = evals_result['validation_0']
    evals_result['test'] = evals_result['validation_1']
    evals_result['valid'] = evals_result['validation_2']

    # Save model
    bst.save_model("XGBRegressor-{}.json".format(XGBname))
    os.system("cp {} inputs_for_models_in_this_dir.py".format(model_inputs_file.__file__))

    fig = plt.figure(figsize=(6, 6))
    plt.subplot(1, 1, 1)
    # plt.title("Values of eval ({})".format(options.eval))
    plt.plot(np.arange(len(evals_result['train'][options.eval])) + 1, evals_result['train'][options.eval], 'b-',
             label='Training Set Eval')
    plt.plot(np.arange(len(evals_result['valid'][options.eval])) + 1, evals_result['valid'][options.eval], 'g-',
             label='Validation Set Eval')
    plt.plot(np.arange(len(evals_result['test'][options.eval])) + 1, evals_result['test'][options.eval], 'r-',
             label='Test Set Eval')
    plt.legend(loc='upper right')
    plt.xlabel('Boosting Iterations')
    plt.ylabel("Values of eval ({})".format(options.eval))
    fig.tight_layout()
    plt.ylim(60, 200)
    if options.eval == 'rmse':
        plt.ylim(90, 150)
    elif options.eval == 'mae':
        plt.ylim(70, 110)
    plt.yscale('log')
    plt.subplots_adjust(left=0.15)
    plt.savefig("History-XGBRegressor-{}.png".format(XGBname))

allowed_channels = ["inclusive", "tt", "mt", "et", "mm", "em", "ee", "lt", "ll"]

for channel in [c for c in options.channels.split(",") if c in allowed_channels]:
    XGB_make_train_predict(df, model_inputs, channel = channel)
