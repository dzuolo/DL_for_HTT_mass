#!/usr/bin/env python3
# -*- coding: utf-8 -*-

import DL_for_HTT.common.NN_settings as NN_default_settings

from optparse import OptionParser
usage = "usage: %prog [options] <input>"
parser = OptionParser(usage=usage)
parser.add_option("-o", "--output", dest = "output",
                  default = "")
parser.add_option("-m", "--minmass", dest = "min_mass",
                  default = NN_default_settings.min_mass)
parser.add_option("-M", "--maxmass", dest = "max_mass",
                  default = NN_default_settings.max_mass)
parser.add_option("-c", "--channels", dest = "channels",
                  default = NN_default_settings.channels)
parser.add_option("-d", "--max_depth", dest = "max_depth",
                  default = 5) # max depth for trees
parser.add_option("-e", "--eta", dest = "eta",
                  default = 0.1) # learning rate
parser.add_option("-n", "--n_estimators", dest = "n_estimators",
                  default = 1000) # max number of estimators
parser.add_option("-s", "--early_stopping_rounds", dest = "early_stopping_rounds",
                  default = 5) # early stopping for estimators
parser.add_option("-E", "--eval", dest = "eval",
                  default = 'rmse')
parser.add_option("-g", "--gamma", dest = "gamma",
                  default = 0)
parser.add_option("-w", "--min_child_weight", dest = "min_child_weight",
                  default = 1)
parser.add_option("-j", "--n_jobs", dest = "n_jobs",
                  default = 1)
parser.add_option("-O", "--objective", dest = "objective",
                  default = 'squarederror')

(options,args) = parser.parse_args()

min_mass = float(options.min_mass)
max_mass = float(options.max_mass)
options.max_depth = int(options.max_depth)
options.eta = float(options.eta)
options.n_estimators = int(options.n_estimators)
if options.early_stopping_rounds not in [None, 'None']:
    options.early_stopping_rounds = int(options.early_stopping_rounds)
else:
    options.early_stopping_rounds = None
options.gamma = float(options.gamma)
options.n_jobs = int(options.n_jobs)
    
input_file = args[0]

print("Selected options are the following:")
for option in ["output", "min_mass", "max_mass", "channels", "max_depth", "eta", "n_estimators", "eval", "gamma", "early_stopping_rounds", "min_child_weight", "n_jobs", "objective"]:
    print("\t{}\t{}".format(option, getattr(options, option)))

print("Input file:")
print("\t{}".format(input_file))

from xgboost import XGBRegressor
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd

# Load data
import os
df = pd.read_hdf(input_file)

# define target and input variables
target = NN_default_settings.target

print("Target: {}".format(target))

inputs = NN_default_settings.inputs

print("Inputs:")
for i in inputs:
    print("\t{}".format(i))
    
# custom objective function
def low_mass(y_true, y_pred):

    grad = y_pred - y_true
    hess = np.ones(len(grad))

    factor = 2
    for low_mass_cut in [100, 120, 130, 150, 200, 400]:
        grad[y_true<low_mass_cut] *= factor
        hess[y_true<low_mass_cut] *= factor

    return grad, hess
    
objectives = {
    'low_mass' : low_mass,
    'linear' : "reg:linear",
    'squarederror' : "reg:squarederror",
    'gamma' : "reg:gamma",
    'tweedie' : "reg:tweedie",
    }
    
def XGB_make_train_predict(df, inputs, channel = "inclusive"):

    params = {
        'max_depth' : options.max_depth,
        'learning_rate' : options.eta,
        'eta' : options.eta, # duplicate for the name
        'n_estimators' : options.n_estimators,
        'gamma' : options.gamma,
        'min_child_weight' : options.min_child_weight,
        'n_jobs' : options.n_jobs,
        'objective' : objectives[options.objective],
        'eval_metric' : options.eval,
        'eval' : options.eval, # duplicate for the name
        'early_stopping_rounds' : options.early_stopping_rounds,
        'es' : options.early_stopping_rounds,
    }

    XGBname_items = []
    if options.output != "":
        XGBname_items.append(options.output)
    XGBname_items.append(channel)
    for param in ['max_depth', 'eta', 'n_estimators', 'es', 'gamma', 'min_child_weight', 'eval']:
        XGBname_items += [param, str(params[param])]
    XGBname = "-".join(XGBname_items)
    XGBname += "-objective-{}".format(options.objective)

    print(XGBname)

    df_select = df
    df_select = df_select.loc[(df_select[target] >= min_mass) & (df_select[target] <= max_mass)]

    if channel in set(df_select['channel_reco']):
        df_select = df_select.loc[(df_select['channel_reco'] == channel)]
    elif channel == "lt":
        df_select = df_select.loc[(df_select['channel_reco'] == "mt") | (df_select['channel_reco'] == "et")]
    elif channel == "ll":
        df_select = df_select.loc[(df_select['channel_reco'] == "mm") | (df_select['channel_reco'] == "em") | (df_select['channel_reco'] == "ee")]

    df_x_train = df_select.loc[(df_select['is_train'] == 1), inputs]
    df_y_train = df_select.loc[(df_select['is_train'] == 1), [target]]
    df_x_valid = df_select.loc[(df_select['is_valid'] == 1), inputs]
    df_y_valid = df_select.loc[(df_select['is_valid'] == 1), [target]]
    df_x_test = df_select.loc[(df_select['is_test'] == 1), inputs]
    df_y_test = df_select.loc[(df_select['is_test'] == 1), [target]]

    print('Size of training set: ', len(df_x_train))
    print('Size of valid set: ', len(df_x_valid))

    if len(df_x_train) == 0 or len(df_x_valid) == 0:
        print("Empty set, aborting...")
        return None, False

    # Create XGBRegressor
    bst = XGBRegressor(**params)
    
    # Create subsamples for evaluation
    eval_set = [(df_x_train, df_y_train), (df_x_test, df_y_test), (df_x_valid, df_y_valid)]
    eval_order = ['train', 'test', 'valid'] # to get back the order of eval_set later

    # Train XGBRegressor
    bst.fit(
        df_x_train, df_y_train,
        verbose=True,
        eval_set = eval_set,
        eval_metric = params['eval_metric'],
        early_stopping_rounds = params['early_stopping_rounds'],
    )
    evals_result = bst.evals_result()
    evals_result['train'] = evals_result['validation_0']
    evals_result['test'] = evals_result['validation_1']
    evals_result['valid'] = evals_result['validation_2']

    # Save model
    bst.save_model("XGBRegressor-{}.json".format(XGBname))

    fig = plt.figure(figsize=(6, 6))
    plt.subplot(1, 1, 1)
    # plt.title("Values of eval ({})".format(options.eval))
    plt.plot(np.arange(len(evals_result['train'][options.eval])) + 1, evals_result['train'][options.eval], 'b-',
             label='Training Set Eval')
    plt.plot(np.arange(len(evals_result['valid'][options.eval])) + 1, evals_result['valid'][options.eval], 'g-',
             label='Validation Set Eval')
    plt.plot(np.arange(len(evals_result['test'][options.eval])) + 1, evals_result['test'][options.eval], 'r-',
             label='Test Set Eval')
    plt.legend(loc='upper right')
    plt.xlabel('Boosting Iterations')
    plt.ylabel("Values of eval ({})".format(options.eval))
    fig.tight_layout()
    plt.ylim(40, 400)
    plt.yscale('log')
    plt.subplots_adjust(left=0.15)
    plt.savefig("History-XGBRegressor-{}.png".format(XGBname))

allowed_channels = ["inclusive", "tt", "mt", "et", "mm", "em", "ee", "lt", "ll"]

for channel in [c for c in options.channels.split(",") if c in allowed_channels]:
    XGB_make_train_predict(df, inputs, channel = channel)
